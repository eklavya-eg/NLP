{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "model_path = \"W:/Models/Sentence Embeddings/universal sentence encoder\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_emb import TensorflowHubEmbeddings\n",
    "embedding = TensorflowHubEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"i like dogs\"\n",
    "s2 = \"i like cannies\"\n",
    "s3 = \"i like candies\"\n",
    "s4 = \"the weather is ugly outside\"\n",
    "e = np.array(model([s1, s2, s3, s4]))\n",
    "print(e)\n",
    "print(np.dot(e[0], e[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from tf_emb import TensorflowHubEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Typesense\n",
    "embedding = TensorflowHubEmbeddings()\n",
    "loader = PyPDFLoader(\"PDF/DPO.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Direct Preference Optimization:\\nYour Language Model is Secretly a Reward Model\\nRafael Rafailov∗†Archit Sharma∗†Eric Mitchell∗†\\nStefano Ermon†‡Christopher D. Manning†Chelsea Finn†\\n†Stanford University‡CZ Biohub\\n{rafailov,architsh,eric.mitchell}@cs.stanford.edu\\nAbstract\\nWhile large-scale unsupervised language models (LMs) learn broad world knowl-\\nedge and some reasoning skills, achieving precise control of their behavior is\\ndifficult due to the completely unsupervised nature of their training. Existing\\nmethods for gaining such steerability collect human labels of the relative quality of\\nmodel generations and fine-tune the unsupervised LM to align with these prefer-\\nences, often with reinforcement learning from human feedback (RLHF). However,\\nRLHF is a complex and often unstable procedure, first fitting a reward model that\\nreflects the human preferences, and then fine-tuning the large unsupervised LM\\nusing reinforcement learning to maximize this estimated reward without drifting\\ntoo far from the original model. In this paper we introduce a new parameterization\\nof the reward model in RLHF that enables extraction of the corresponding optimal\\npolicy in closed form, allowing us to solve the standard RLHF problem with only a\\nsimple classification loss. The resulting algorithm, which we call Direct Prefer-\\nence Optimization (DPO), is stable, performant, and computationally lightweight,\\neliminating the need for sampling from the LM during fine-tuning or performing\\nsignificant hyperparameter tuning. Our experiments show that DPO can fine-tune\\nLMs to align with human preferences as well as or better than existing methods.\\nNotably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sen-\\ntiment of generations, and matches or improves response quality in summarization\\nand single-turn dialogue while being substantially simpler to implement and train.\\n1 Introduction\\nLarge unsupervised language models (LMs) trained on very large datasets acquire surprising capabili-\\nties [ 11,7,40,8]. However, these models are trained on data generated by humans with a wide variety\\nof goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for\\nexample, while we may want our AI coding assistant to understand common programming mistakes\\nin order to correct them, nevertheless, when generating code, we would like to bias our model toward\\nthe (potentially rare) high-quality coding ability present in its training data. Similarly, we might want\\nour language model to be aware of a common misconception believed by 50% of people, but we\\ncertainly do not want the model to claim this misconception to be true in 50% of queries about it!\\nIn other words, selecting the model’s desired responses and behavior from its very wide knowledge\\nand abilities is crucial to building AI systems that are safe, performant, and controllable [ 26]. While\\nexisting methods typically steer LMs to match human preferences using reinforcement learning (RL),\\n∗Equal contribution; more junior authors listed earlier.\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2305.18290v2  [cs.LG]  13 Dec 2023'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 100, chunk_overlap=30)\n",
    "docs = splitter.split_documents(pages)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Typesense.from_documents(\n",
    "    docs,\n",
    "    embedding,\n",
    "    typesense_client_params={\n",
    "        \"host\": \"localhost\",  # Use xxx.a1.typesense.net for Typesense Cloud\n",
    "        \"port\": \"8108\",  # Use 443 for Typesense Cloud\n",
    "        \"protocol\": \"http\",  # Use https for Typesense Cloud\n",
    "        \"typesense_api_key\": \"xyz\",\n",
    "        \"typesense_collection_name\": \"lang-chain\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# docker run -p 8108:8108 -v %cd%/typesense-data:/data typesense/typesense:26.0 --data-dir /data --api-key=\"xyz\" --enable-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'}), Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'}), Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'}), Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'}), Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'}), Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'}), Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'}), Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'}), Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'}), Document(page_content='he sang befor e Emper or Akbar , who was so impr essed by', metadata={'page': 2, 'source': 'Docs\\\\fepw104.pdf'})]\n",
      "[Document(page_content='Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for', metadata={'page': 1, 'source': 'PDF/DPO.pdf'}), Document(page_content='over-optimization manifest in the direct preference optimization setting, and is the slight', metadata={'page': 9, 'source': 'PDF/DPO.pdf'}), Document(page_content='a simple approach for policy optimization', metadata={'page': 3, 'source': 'PDF/DPO.pdf'}), Document(page_content='effectiveness of each algorithm in optimizing the constrained reward maximization objective, in the', metadata={'page': 6, 'source': 'PDF/DPO.pdf'}), Document(page_content='4 Direct Preference Optimization', metadata={'page': 3, 'source': 'PDF/DPO.pdf'}), Document(page_content='Direct Preference Optimization:\\nYour Language Model is Secretly a Reward Model', metadata={'page': 0, 'source': 'PDF/DPO.pdf'}), Document(page_content='a theoretically-justified approach to optimizing relative preferences without RL.', metadata={'page': 2, 'source': 'PDF/DPO.pdf'}), Document(page_content='explicitly make the optimal policy', metadata={'page': 5, 'source': 'PDF/DPO.pdf'}), Document(page_content='quality of the reward model from the PPO optimization, but is computationally impractical even for', metadata={'page': 7, 'source': 'PDF/DPO.pdf'}), Document(page_content='rϕ(y, x). With some algebra this leads to the optimization objective:\\nmax\\nπθEπθ(y|x)\\x14', metadata={'page': 5, 'source': 'PDF/DPO.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is idea behind direct preference optimization\"\n",
    "found_docs = docsearch.similarity_search(query)\n",
    "print(found_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typesense\n",
    "\n",
    "client = typesense.Client({\n",
    "  'nodes': [{\n",
    "    'host': 'localhost',\n",
    "    'port': '8108',\n",
    "    'protocol': 'http'\n",
    "  }],\n",
    "  'api_key': 'xyz',\n",
    "  'connection_timeout_seconds': 2\n",
    "})\n",
    "\n",
    "\n",
    "key = client.keys.create({\n",
    "  \"description\": \"Admin key.\",\n",
    "  \"actions\": [\"*\"],\n",
    "  \"collections\": [\"*\"]\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma/'\n",
    "# !rm -rf ./docs/chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embedding,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is disadvantage of reinforcement learning form human feedback\"\n",
    "docs = vectordb.similarity_search(question,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
