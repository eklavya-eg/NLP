{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"PDF/DPO.pdf\")\n",
    "pages = loader.load()\n",
    "print(pages.__len__())\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "docs = splitter.split_documents(pages)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Direct Preference Optimization:\\nYour Language Model is Secretly a Reward Model\\nRafael Rafailov∗†Archit Sharma∗†Eric Mitchell∗†\\nStefano Ermon†‡Christopher D. Manning†Chelsea Finn†\\n†Stanford University‡CZ Biohub\\n{rafailov,architsh,eric.mitchell}@cs.stanford.edu\\nAbstract\\nWhile large-scale unsupervised language models (LMs) learn broad world knowl-\\nedge and some reasoning skills, achieving precise control of their behavior is\\ndifficult due to the completely unsupervised nature of their training. Existing\\nmethods for gaining such steerability collect human labels of the relative quality of\\nmodel generations and fine-tune the unsupervised LM to align with these prefer-\\nences, often with reinforcement learning from human feedback (RLHF). However,\\nRLHF is a complex and often unstable procedure, first fitting a reward model that\\nreflects the human preferences, and then fine-tuning the large unsupervised LM\\nusing reinforcement learning to maximize this estimated reward without drifting\\ntoo far from the original model. In this paper we introduce a new parameterization\\nof the reward model in RLHF that enables extraction of the corresponding optimal\\npolicy in closed form, allowing us to solve the standard RLHF problem with only a\\nsimple classification loss. The resulting algorithm, which we call Direct Prefer-\\nence Optimization (DPO), is stable, performant, and computationally lightweight,\\neliminating the need for sampling from the LM during fine-tuning or performing\\nsignificant hyperparameter tuning. Our experiments show that DPO can fine-tune\\nLMs to align with human preferences as well as or better than existing methods.\\nNotably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sen-\\ntiment of generations, and matches or improves response quality in summarization\\nand single-turn dialogue while being substantially simpler to implement and train.\\n1 Introduction\\nLarge unsupervised language models (LMs) trained on very large datasets acquire surprising capabili-\\nties [ 11,7,40,8]. However, these models are trained on data generated by humans with a wide variety\\nof goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for\\nexample, while we may want our AI coding assistant to understand common programming mistakes\\nin order to correct them, nevertheless, when generating code, we would like to bias our model toward\\nthe (potentially rare) high-quality coding ability present in its training data. Similarly, we might want\\nour language model to be aware of a common misconception believed by 50% of people, but we\\ncertainly do not want the model to claim this misconception to be true in 50% of queries about it!\\nIn other words, selecting the model’s desired responses and behavior from its very wide knowledge\\nand abilities is crucial to building AI systems that are safe, performant, and controllable [ 26]. While\\nexisting methods typically steer LMs to match human preferences using reinforcement learning (RL),\\n∗Equal contribution; more junior authors listed earlier.\\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2305.18290v2  [cs.LG]  13 Dec 2023' metadata={'source': 'PDF/DPO.pdf', 'page': 0}\n",
      "page_content='Direct Preference Optimization:\\nYour Language Model is Secretly a Reward Model\\nRafael Rafailov∗†Archit Sharma∗†Eric Mitchell∗†\\nStefano Ermon†‡Christopher D. Manning†Chelsea Finn†\\n†Stanford University‡CZ Biohub\\n{rafailov,architsh,eric.mitchell}@cs.stanford.edu\\nAbstract\\nWhile large-scale unsupervised language models (LMs) learn broad world knowl-\\nedge and some reasoning skills, achieving precise control of their behavior is\\ndifficult due to the completely unsupervised nature of their training. Existing\\nmethods for gaining such steerability collect human labels of the relative quality of\\nmodel generations and fine-tune the unsupervised LM to align with these prefer-\\nences, often with reinforcement learning from human feedback (RLHF). However,\\nRLHF is a complex and often unstable procedure, first fitting a reward model that\\nreflects the human preferences, and then fine-tuning the large unsupervised LM\\nusing reinforcement learning to maximize this estimated reward without drifting' metadata={'source': 'PDF/DPO.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EKLAVYA\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n\u001b[1;32m----> 2\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi am feeling good.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding\u001b[38;5;241m.\u001b[39membed_query(text))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pydantic\\main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import \n",
    "embedding = OpenAIEmbeddings()\n",
    "text = \"i am feeling good.\"\n",
    "print(embedding.embed_query(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"M:/Models/Sentence Embeddings/\")\n",
    "embeddings = embed([\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I am a sentence for which I would like to get its embedding\"])\n",
    "\n",
    "print(embeddings)\n",
    "\n",
    "# The following are example embedding output of 512 dimensions per sentence\n",
    "# Embedding for: The quick brown fox jumps over the lazy dog.\n",
    "# [-0.03133016 -0.06338634 -0.01607501, ...]\n",
    "# Embedding for: I am a sentence for which I would like to get its embedding.\n",
    "# [0.05080863 -0.0165243   0.01573782, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\EKLAVYA\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\EKLAVYA\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\EKLAVYA\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\EKLAVYA\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\EKLAVYA\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.03133017 -0.06338634 -0.01607501 ... -0.03242778 -0.0457574\n",
      "   0.05370456]\n",
      " [ 0.0508086  -0.01652434  0.01573779 ...  0.00976657  0.03170121\n",
      "   0.01788118]], shape=(2, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\")\n",
    "embeddings = embed([\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I am a sentence for which I would like to get its embedding\"])\n",
    "\n",
    "print(embeddings)\n",
    "\n",
    "# The following are example embedding output of 512 dimensions per sentence\n",
    "# Embedding for: The quick brown fox jumps over the lazy dog.\n",
    "# [-0.03133016 -0.06338634 -0.01607501, ...]\n",
    "# Embedding for: I am a sentence for which I would like to get its embedding.\n",
    "# [0.05080863 -0.0165243   0.01573782, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
